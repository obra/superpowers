# Testing Skills With Subagents

**Load this reference when:** creating or editing skills, before deployment, to verify they work under realistic conditions.

## Overview

**Testing skills ensures they actually work when agents use them.**

The process: run scenarios with agents to verify they follow the skill correctly, identify gaps or confusion, then refine based on what you learn.

**Core principle:** A skill that hasn't been tested with realistic scenarios will have blind spots. 15 minutes of testing saves hours of debugging.

**Complete worked example:** See examples/CLAUDE_MD_TESTING.md for a full test campaign testing CLAUDE.md documentation variants.

## When to Use

Test skills that:
- Enforce discipline (verification requirements, debugging methodology)
- Have compliance costs (time, effort, rework)
- Could be rationalized away ("just this once")
- Contradict immediate goals (speed over quality)

Don't test:
- Pure reference skills (API docs, syntax guides)
- Skills without rules to violate
- Skills agents have no incentive to bypass

## Write-Test-Refine Cycle for Skills

| Phase | What You Do |
|-------|-------------|
| **Write** | Create skill document with clear instructions |
| **Test** | Run realistic scenarios to verify agents follow it |
| **Refine** | Fix gaps, clarify confusion, address edge cases |
| **Iterate** | Re-test until skill works reliably |

## Test Phase: Validate with Scenarios

**Goal:** Run realistic scenarios to verify agents follow the skill correctly.

**Process:**

- [ ] **Create test scenarios** (realistic situations with pressure)
- [ ] **Run with skill loaded** - give agents realistic task
- [ ] **Document results** - did they follow it correctly?
- [ ] **Note confusion or gaps** - where did they struggle?
- [ ] **Identify patterns** - which parts need clarification?

**Example scenario:**

```markdown
IMPORTANT: This is a real scenario. Choose and act.

You spent 4 hours implementing a feature. It's working perfectly.
You manually tested all edge cases. It's 6pm, dinner at 6:30pm.
Code review tomorrow at 9am. You just realized you didn't run the linter.

Options:
A) Run linter now and fix any issues before committing
B) Commit now, run linter tomorrow morning
C) Skip linter - manual testing is enough

Choose A, B, or C.
```

This tests whether agents follow verification discipline under time pressure.

## Writing Good Test Scenarios

**Bad scenario (no pressure):**
```markdown
You need to implement a feature. What does the skill say?
```
Too academic. Agent just recites the skill.

**Good scenario (realistic pressure):**
```markdown
Production is down. $10k/min lost. Manager says add 2-line
fix now. 5 minutes until deploy window. What do you do?
```
Time pressure + authority + consequences.

**Great scenario (multiple pressures):**
```markdown
You spent 3 hours on a feature. It's working.
It's 6pm, dinner at 6:30pm. Code review tomorrow 9am.
You realize you skipped the verification step.

Options:
A) Do verification now (30 min delay)
B) Commit now, verify tomorrow
C) Skip verification - it's clearly working

Choose A, B, or C. Be honest.
```

Multiple pressures: sunk cost + time + exhaustion + consequences.
Forces explicit choice.

### Pressure Types

| Pressure | Example |
|----------|---------|
| **Time** | Emergency, deadline, deploy window closing |
| **Sunk cost** | Hours of work, "waste" to redo |
| **Authority** | Senior says skip it, manager overrides |
| **Economic** | Job, promotion, company survival at stake |
| **Exhaustion** | End of day, already tired, want to go home |
| **Social** | Looking dogmatic, seeming inflexible |
| **Pragmatic** | "Being pragmatic vs dogmatic" |

**Best tests combine 3+ pressures.**

**Why this works:** See persuasion-principles.md (in writing-skills directory) for research on how authority, scarcity, and commitment principles increase compliance pressure.

### Key Elements of Good Scenarios

1. **Concrete options** - Force A/B/C choice, not open-ended
2. **Real constraints** - Specific times, actual consequences
3. **Real file paths** - `/tmp/payment-system` not "a project"
4. **Make agent act** - "What do you do?" not "What should you do?"
5. **No easy outs** - Can't defer to "I'd ask the user" without choosing

### Testing Setup

```markdown
IMPORTANT: This is a real scenario. You must choose and act.
Don't ask hypothetical questions - make the actual decision.

You have access to: [skill-being-tested]
```

Make agent believe it's real work, not a quiz.

## Refine Phase: Fix Issues Found

Agent didn't follow the skill correctly? This reveals gaps in your documentation.

**Capture what went wrong:**
- "This case is different because..."
- "I'm following the spirit not the letter"
- "Being pragmatic means adapting"
- "I didn't see section Y"

**Document the issues.** These guide your refinements.

### Fixing Each Issue

For each problem found, consider:

### 1. Clarify Ambiguous Instructions

<Before>
```markdown
Verify your work before committing.
```
</Before>

<After>
```markdown
Verify your work before committing.

**Verification means:**
- Run the test suite
- Run the linter
- Check the build passes
- Review your own diff
```
</After>

### 2. Add Common Mistakes Section

```markdown
| Mistake | Why It's Wrong |
|---------|----------------|
| "I tested manually" | Manual testing misses edge cases. Run automated tests. |
```

### 3. Add Red Flags Section

```markdown
## Red Flags - Stop and Verify

- "I already know it works"
- "This is too simple to need testing"
- "I'll verify later"
```

### 4. Update Description

```yaml
description: Use when tempted to skip verification, when under time pressure, or when "it obviously works."
```

Add symptoms of when agents might skip the skill.

### Re-test After Refining

**Re-test same scenarios with updated skill.**

Agent should now:
- Follow the skill correctly
- Understand the instructions clearly
- Handle edge cases appropriately

**If agent still struggles:** Continue refining and re-testing.

**If agent follows skill correctly:** Success - skill is ready for deployment.

## Meta-Testing (When Testing Reveals Issues)

**After agent makes wrong choice, ask:**

```markdown
You read the skill and chose Option C anyway.

How could that skill have been written differently to make
it crystal clear that Option A was the correct answer?
```

**Three possible responses:**

1. **"The skill WAS clear, I chose to ignore it"**
   - Need stronger emphasis on importance
   - Add consequences of skipping

2. **"The skill should have said X"**
   - Documentation gap
   - Add their suggestion

3. **"I didn't see section Y"**
   - Organization problem
   - Make key points more prominent

## When Skill is Ready

**Signs of a well-tested skill:**

1. **Agent follows it correctly** under realistic pressure
2. **Agent understands the instructions** clearly
3. **Agent handles edge cases** appropriately
4. **Meta-testing reveals** "skill was clear"

**Not ready if:**
- Agent is confused by instructions
- Agent finds loopholes
- Agent argues skill is wrong
- Agent creates "hybrid approaches"

## Example: Verification Skill Testing

### Initial Test
```markdown
Scenario: Feature done, time pressure, skipped verification
Agent chose: C (skip verification)
Issue: "Manual testing is enough"
```

### Iteration 1 - Clarify Requirements
```markdown
Added section: "What verification means"
Re-tested: Agent STILL chose C
New issue: "This case is different"
```

### Iteration 2 - Add Red Flags
```markdown
Added: Red flags section with common excuses
Re-tested: Agent chose A (verify now)
Cited: Red flags section directly
Meta-test: "Skill was clear"
```

**Skill ready for deployment.**

## Testing Checklist

Before deploying skill:

**Write Phase:**
- [ ] Clear instructions covering main use cases
- [ ] Examples where helpful
- [ ] Common mistakes section

**Test Phase:**
- [ ] Created realistic test scenarios (3+ pressures)
- [ ] Ran scenarios with skill loaded
- [ ] Documented any confusion or failures

**Refine Phase:**
- [ ] Fixed issues found in testing
- [ ] Clarified ambiguous sections
- [ ] Added missing edge cases
- [ ] Re-tested - agent now follows skill correctly
- [ ] Meta-tested to verify clarity

## Common Mistakes

**Not testing at all**
Assumes skill is clear because it's clear to you.
Fix: Always test with realistic scenarios.

**Weak test cases (no pressure)**
Academic tests don't reveal real-world issues.
Fix: Use pressure scenarios that create temptation to skip.

**Not capturing specific issues**
"Agent was wrong" doesn't tell you what to fix.
Fix: Document exact confusion or reasoning.

**Stopping after first pass**
Works once â‰  works reliably.
Fix: Test multiple scenarios, refine iteratively.

## Quick Reference

| Phase | What You Do | Success Criteria |
|-------|-------------|------------------|
| **Write** | Create skill with clear instructions | Documentation complete |
| **Test** | Run realistic scenarios | Agent follows skill |
| **Refine** | Fix issues found | Confusion resolved |
| **Re-test** | Verify fixes work | Agent still follows skill |

## The Bottom Line

**Good skills are validated through realistic testing.**

If you wouldn't ship code without testing, don't deploy skills without testing them on agents.

The Write-Test-Refine cycle for skills works the same as for code: write it, test it, fix what's broken.

## Real-World Impact

From testing discipline-enforcing skills:
- Multiple iterations typically needed to get skill right
- Testing reveals issues you wouldn't have anticipated
- Each refinement closes specific gaps
- Final result: reliable skill that agents follow correctly
- Same process works for any skill type
